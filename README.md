# Polynomial-Regression-and-Stochastic-Gradient-Descent
In this assignment, we focused on polynomial regression and stochastic gradient descent (SGD) to understand the effects of model complexity, regularization, and optimization techniques in machine learning.

---

### Key Components:

1. **Polynomial Regression**:
   - We implemented polynomial regression models of varying degrees (3, 5, 21) to predict target values from a dataset.
   - Using **train and test Mean Squared Errors (MSE)**, we analyzed how the model degree influences performance and the balance between **bias and variance**.
   - **Regularization** (L2 regularization) was introduced to mitigate overfitting in complex models, improving generalization.

2. **Learning Curves**:
   - We plotted learning curves to visualize how model performance changes with increasing training set size. This helped us understand the **bias-variance tradeoff** and identify the ideal model complexity based on the dataset size.

3. **Stochastic Gradient Descent (SGD)**:
   - We implemented SGD for a non-linear regression model, updating coefficients based on various learning rates and regularization strengths.
   - Using data from **SGD_samples.csv**, we optimized the model over 15 epochs and tracked MSE and parameter updates.
   - The effects of **learning rate** and **regularization** on the optimization process were visualized, showing their impact on convergence and final model performance.

---

### Conclusion:
Through this project, we gained a deeper understanding of how model complexity, regularization, and optimization algorithms interact to influence predictive performance. We demonstrated the importance of hyperparameter tuning and visualizing learning curves to select the best model for a given problem.

### Group Members:
- **Sushanth Ravichandran**
- **Sankeerth Viswanadhuni**

